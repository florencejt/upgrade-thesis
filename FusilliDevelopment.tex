\chapter{Fusilli: Developing a Data Fusion Python Library}
\label{fusilli_development}

\textbf{Linking sentence from Cox chapter:
We've seen how multimodal data affects a survival Cox model in MND.
What about machine learning for multimodal data?}

This chapter discusses multimodal data fusion in more detail and describe the development of Fusilli, a Python package for multimodal data fusion experimentation and analysis.

\section{Introduction}

Multimodal data fusion is the process of combining data from different sources to make predictions or decisions, often through the use of deep learning.
The goal of combining different modalities is to improve the performance of a model by leveraging the relevant information from each modality and fusing them in a way that improves the model's performance.
There are many research fields where multimodal data fusion is used, such as in agriculture to predict crop yields and detect diseases~\cite{s.s.gopiMultimodalMachineLearning2023, patilRiceFusionMultimodalityData2022}, in disaster management to analyse response scenarios from audio and social media posts~\cite{algiriyageMultisourceMultimodalData2021}, and in robotics to help direct the robots with multiple sensors~\cite{duanMultimodalSensorsMLBased2022}.
Moreover, the types of models used in multimodal data fusion can vary a lot, from geometric deep learning to relatively simple neural network architectures~\cite{cuiDeepMultimodalFusion2022}.

%In the context of my PhD, I am investigating multimodal data fusion for MND prognosis prediction.
%There has been minimal research on deep learning for prognosis prediction in MND~\cite{pancottiDeepLearningMethods2022, mullerExplainableModelsDisease2021}, and only one model created for deep-learning based multimodal data fusion~\cite{vanderburghDeepLearningPredictions2017}.
%However, there are many data fusion models that have been used in other fields that could be directly applicable to MND .
%There have been systematic reviews on the topic of data fusion that compare models, but only qualitatively~\cite{cuiDeepMultimodalFusion2022, gaoSurveyDeepLearning2020, stahlschmidtMultimodalDeepLearning2022, yanDeepMultiviewLearning2021}.
%Therefore, with limited knowledge from literature on the best models for MND data, it is important to experiment with a wide variety of models to find the best one for the task.

My PhD investigates multimodal data fusion for predicting MND prognosis, an area with minimal deep learning research~\cite{pancottiDeepLearningMethods2022, mullerExplainableModelsDisease2021} and only one model specially-created for deep-learning based multimodal data fusion~\cite{vanderburghDeepLearningPredictions2017}.
Despite systematic reviews on the topic attempting qualitative comparison between models~\cite{cuiDeepMultimodalFusion2022, gaoSurveyDeepLearning2020, stahlschmidtMultimodalDeepLearning2022, yanDeepMultiviewLearning2021}, a lack of quantitative comparison necessitates experimentation with many models from other fields to identify the most effective for MND prognosis prediction.

%Obtaining this wide variety of models to experiment with, however, is not a straightforward process for a number of reasons.
%Firstly, the terminology used to describe what I have chosen to call ``multimodal data fusion`` varies widely, with terms such as but not limited to multi-view, cross-heterogeneous, multi-source, and integrated learning.
%Secondly, the studies that introduce these models often do not include code for the reader to run the model.
%Morever, when code is included, it is often written in different languages, with varying guidance, quality, and maintenance.
However, acquring a diverse range of models for experimentation is made difficult by the use of variable terminology and the common absence of maintained, quality code available in studies.

A way to address these problems is to create a curated collection of models for somebody interested in multimodal data fusion to consult.
As far as I am aware, there are three Python packages that house collections of deep learning based data fusion models: ``Multi-view-AE``~\cite{aguilaMultiviewAEPythonPackage2023}, ``CCA-Zoo``~\cite{chapmanCCAZooCollectionRegularized2021}, and ``pytorch-widedeep``~\cite{zaurinPytorchwidedeepFlexiblePackage2023}.
However, each of these packages only includes models with specific frameworks (autoencoders, CCA, and Google's ``wide and deep`` models, respectively), which limits the variety of models available for comparison.

Therefore, I aimed to develop a Python package for training and comparing multimodal data fusion models with any architecture.
This Python package is named Fusilli, as a portmanteau of ``fuse easily``.
Fusilli works by taking the user's multimodal data and training it on a variety of models, and then comparing the models' performances.

\section{Development and Implementation}

\subsection{Software Design Choices}

Before developing Fusilli, the following design goals were set to ensure the package would be useful for a wide range of users and tasks, as well as for my own research.\newline\newline
\noindent\textbf{Modularity}: Fusilli should be modular, meaning that the various functionalities within the package should be independent of each other.
This would allow for easy addition of new models in the future and easy adjustments to the package's functionality.\newline\newline
\noindent\textbf{Beginner-friendly and expert-friendly}:  Fusilli should be beginner-friendly, with users able to compare the different models without needing expertise in deep learning or Python programming.
This would make it different from other similar packages, which require the user to set up their own experiments.

On the other hand, Fusilli should also be expert-friendly, with users who are more capable being able to change the training parameters, modify the models, and access the trained models for further experiments.\newline\newline
\noindent\textbf{Wide applicability}:
Fusilli should include a wide variety of models, to ensure that the best model for a given task can be found.
Moreover, this would ensure that the package is useful for a wide range of users, as different users may have different requirements for model architectures based on their task and data.\newline\newline
\noindent\textbf{Support for two modalities}: The models in Fusilli will support data fusion between either two types of tabular data (e.g. clinical data and brain region volumes data) or between an image and tabular data (e.g. MRI images and clinical data).
Again, for wide applicability, Fusilli should be able to handle two-dimensional and three-dimensional images, and tabular data of any size.\newline

\noindent\textbf{Support for different prediction tasks}: The prediction tasks that Fusilli should support are regression and classification.
From the literature, these are the most common tasks for multimodal data fusion, and are the tasks that I am interested in for my own research.\newline


\subsection{Implementation}

Fusilli was implemented in Python, using the PyTorch and PyTorch Lightning libraries for deep learning.
This is because PyTorch is a popular library for deep learning, and is known for its flexibility and ease of use.

\noindent The user must specify:
\begin{itemize}
  \setlength\itemsep{-0.5em}
    \item \textbf{Data}: The user's data, which must be in the form of a .csv file for tabular data, and .pt files for images.
    \item \textbf{Task}: The task that the user wants to perform, which can be either regression or classification (binary or multiclass).
    \item \textbf{Models}: The models that the user wants to compare, which can be any of the models included in Fusilli.
    \item \textbf{Output}: The output directory for the trained models and the results of the experiments.
\end{itemize}

\noindent The user can also specify experiment specifics, which are set to default values if not specified.
These are:
\begin{itemize}
    \setlength\itemsep{-0.5em}
    \item Training and validation data splits.
    \item Maximum number of epochs to train for, including early stopping.
    \item Any model hyperparameters or architectures that the user wants to modify.
    \item Batch size.
\end{itemize}

\noindent After the user has specified these, the user calls two functions in Fusilli to train the models:
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \texttt{prepare\_fusion\_data()}: This function prepares the data for training, including splitting it into training and validation sets and running any model-specific data preparation steps.
    \item \texttt{train\_and\_save\_models()}: This function trains the models on the user's data, and outputs the trained models.
\end{itemize}

Finally, the user can call functions to evaluate a single model or compare the models, which will output the performance of the models on the user's validation data or external test data.
The evaluation figures are saved in the output directory, and the user can also access the trained models for further analysis.

\subsection{Fusion Methods}

A literature search was conducted to find models that could be included in Fusilli.
The search criteria aimed to capture papers that mentioned machine learning, multimodality, and image and tabular data, with variants of these terms used to capture a wide range of papers.
The resulting papers were checked for relevance and papers were discarded which use the same model as another paper, which happened frequently.

The models were categorised based on the taxonomy defined by Cui and colleagues in their review on data fusion methods for diagnosis and prognosis~\cite{cuiDeepMultimodalFusion2022}.
This taxonomy includes the following categories:
\begin{itemize}
\setlength\itemsep{-0.5em}
    \item \textbf{Operation-based}: Models that fuse data based on operations such as concatenation, addition, or multiplication.
    This can be done at any point in the model, such as at the input, hidden layers, or output.
    \item \textbf{Attention-based}: Models that use attention mechanisms to weight the importance of different modalities.
    \item \textbf{Graph-based}: Models with a graph structure, such as graph convolutional networks.
    \item \textbf{Subspace-based}: Models that project the data into a joint lower-dimensional space, possibly with deep learning methods such as autoencoders.
    \item \textbf{Tensor-based}: Models that use tensor operations to fuse the data to capture inter- and intra-modality correlations.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/cui_diagram}
    \caption{Fusilli's architecture-based taxonomy of multimodal data fusion models. Diagram taken from Cui et al.~\cite{cuiDeepMultimodalFusion2022}.}
    \label{fig:fusilli_taxonomy}
\end{figure}

Figure~\ref{fig:fusilli_taxonomy} shows the architecture-based taxonomy of multimodal data fusion models, taken from Cui et al.~\cite{cuiDeepMultimodalFusion2022}.

Another common categorisation is ``early``, ``intermediate``, and ``late`` fusion, which refers to the point in the model where the data is fused.
However, this categorisation is too simplistic for the variety of models architectures in Fusilli, and so the architecture-based taxonomy was chosen for its more detailed categories.

More models were found in the literature than were included in Fusilli due to the time constraints of the project, but the models included in Fusilli were chosen based on their popularity, availability of code, and the variety of architectures they represent.
Moreover, unimodal benchmarks were included in Fusilli to allow for comparison between the multimodal models and unimodal models.

\noindent \texbf{Table of the models with link to documentation in the caption}


\section{Results}




\subsection{Diagram of workflow}

\subsection{Example usage and outputs}
Quick-start script with comments going through each line

Figures with output figures from the example notebooks

\subsection{Documentation}
Examples, templates for contributing, etc


\subsection{Reception}
\begin{itemize}
    \item JOSS paper - under review 
    \item GitHub stars and forks and articles
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item Implemented a wide variety of models
    \item It will help people to know if data fusion is useful for their task and, if so, which model is best for their task
    \item Limitation and future work: Data inputs - images have to be .pt files and tabular data has to be .csv files. Would be nice to extend data inputs to be jpegs or niis for people less comfortable with Python.
    \item Limitation and future work: Only 2 modalities - could be extended to more
\end{itemize}

\section{Conclusion}
TBC...
% \begin{itemize}
%     \item Fusilli is a Python package for multimodal data fusion experimentation and analysis, specifically tackling the problem of lack of ways to compare models and lack of standardisation in the field.
%     \item I specifically made it for my PhD in fusing different data modalities in MND prognosis prediction
%     \item Link to next chapter on PPMI, ADNI, MIMIC: data fusion can be applied to any task where there are multiple data modalities describing one thing
%     \item Before going into MND, is there a clear benefit to different types of multimodal data fusion? Is there a consensus on the best approaches for different tasks?
%     \item MND sample size is small so we want to make sure we're using the best models by testing them on larger datasets with different applications
% \end{itemize}
