\chapter{Comparison of Multimodal Data Fusion Models for Predicting Survival Classification in Motor Neuron Disease}
\label{fusilli_on_mnd}

\section{Introduction}
Only one deep learning model has been used to predict survival in MND through clinical data and neuroimaging data~\cite{vanderburghDeepLearningPredictions2017}.
This lack of research in multimodal data fusion in MND makes it difficult to conclude whether multimodal data is useful in predicting survival in MND .
Additionally, we have shown that many different multimodal data fusion methods have been developed for other research applications.
In Chapter~\ref{fusilli_development}, we developed a package called Fusilli to compare the performance of multimodal data fusion methods, approximately half of which are designed to combine two tabular modalities~\cite{townendFlorencejtFusilliFusilli2024}.

In this chapter, we will use Fusilli to compare the performance of different multimodal data fusion methods on predicting survival in MND patients using clinical and imaging data.
The aims of this work were to, firstly, assess the effect of different data fusion model architectures on prognostic performance, and secondly, to assess the value of baseline clinical and neuroimaging data in MND prognosis prediction.

\section{Data}

The data used in this analysis is from two studies: University College London Queen's Square Institute of Neurology's ALS Biomarkers Study~\cite{UKMNDCSG} and Ospedale San Raffaele's MND cohort.
Both of these datasets contain clinical data from the diagnostic visit and brain MRI data.

For patients to be included in the analysis, they must have received a diagnosis of MND and had an outcome of interest (death or tracheostomy).
In the ALS Biomarkers Study, the outcome of interest was death, whereas in the Ospedale San Raffaele's MND cohort, the outcome of interest was death or tracheostomy.
Unfortunately, the Ospedale San Raffaele's MND cohort did not specify the date of death, so we have assumed that the date of death was the date of tracheostomy.

Patients in this analysis must have had non-missing data for age at diagnosis, sex, date of diagnosis, date of death, date of symptom onset, site of onset, and baseline ALSFRS-R .
Additionally, patients must have had a T1-weighted or T2-weighted MRI within 12 months before or after their date of diagnosis.

The final dataset contained 110 MND patients.
The patients in the cohort were split into two groups based on the median survival time: short survival group (less than 24 months) and long survival group (more than 24 months).
The median was used, rather than a clinically relevant cut-off, as the sample size was small and having balanced classes reduces the risk of overfitting.
Moreover, the median survival time in our data of 24 months is a reasonable cut-off for short and long survival in MND~\cite{feldmanAmyotrophicLateralSclerosis2022}.

\subsection{Clinical Data}

The clinical variables chosen to be included in the analysis are based on the variables used in the ENCALS model~\cite{westenengPrognosisPatientsAmyotrophic2018}.
El Escorial criteria and FVC were not included in the analysis as they were not available in the Ospedale San Raffaele's MND cohort.
Features with missing data after applying the inclusion criteria were the co-presence of FTD and the C9orf72 mutation.
Where these features were missing, they were assumed to be negative.
Moreover, where the MND type was missing, it was assumed to be ALS, as it is the most common.


\begin{table}
    \centering
    \caption{Differences in clinical demographics between the long and short survival groups. PRB is progression rate to baseline, calculated as the rate of decline of ALSFRS-R between symptom onset and diagnosis.\\
    *Chi-square test, \textdagger Fisher's exact test, $\ddagger$ Two-sample t-test.}
    \label{tab:clinical_demographics}
    \begin{tabular}{|p{4.3cm}|llll|}
    \hline
                                                        & \textbf{Overall}     & \textbf{Short}        & \textbf{Long}         & \textbf{P-Value}   \\
    \hline
     n                                                  & 110         & 55         & 55          &           \\ \hline
    \textbf{Categorical, n (\%)}                                &             &            &             &           \\ \hline
     Sex (Male)                                     & 52 (47.3)   & 27 (49.1)  & 25 (45.5)   & 0.849*     \\ \hline
     Bulbar Onset                          & 31 (28.2)   & 20 (36.4)  & 11 (20.0)  & 0.090*     \\\hline
     FTD                      & 32 (29.1)   & 24 (43.6)  & 8 (14.5)   & \textbf{0.002}*     \\\hline
     C9orf72                             & 7 (6.4)     & 2 (3.6)    & 5 (9.1)   & 0.438\textdagger     \\\hline
    ALS                               & 96 (87.3)   & 51 (92.7)  & 45 (81.8)   & 0.153*     \\\hline
    \textbf{Continuous, mean (SD)}                              &             &            &             &           \\ \hline
     ALSFRS-R                                & 37.5 (7.2)  & 36.3 (7.2) & 38.7 (7.0)  & $0.081\ddagger$    \\\hline
     \makecell[l]{Diagnostic Delay, mo}                 & 12.5 (12.0) & 10.0 (9.8) & 14.9 (13.4) & \textbf{0.031}$\ddagger$     \\\hline
     \makecell[l]{Age at Diagnosis, yr}                   & 63.2 (11.8) & 69.1 (9.1) & 57.3 (11.3) & \textbf{\ensuremath{<}0.001}$\ddagger$    \\\hline
     \makecell[l]{PRB (points/month)}       & 1.4 (1.7)   & 2.0 (2.2)  & 0.9 (0.7)   & \textbf{0.001}$\ddagger$      \\\hline
     \makecell[l]{Survival, mo}                          & 29.3 (23.2) & 12.3 (6.4) & 46.4 (21.3) & \textbf{\ensuremath{<}0.001}$\ddagger$     \\\hline
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Differences in clinical demographics between the two data sites: the ALS Biomarkers Study from University College London and Ospedale San Raffaele.
    PRB is progression rate to baseline, calculated as the rate of decline of ALSFRS-R between symptom onset and diagnosis.\\
    *Chi-square test, \textdagger Fisher's exact test, $\ddagger$ Two-sample t-test.}
    \label{tab:clinical_demographics_site}
    \begin{tabular}{|p{4.3cm}|llll|}
    \hline
                                                       & Overall     & \makecell[l]{ALS Biomarkers \\ Study}       & \makecell[l]{Ospedale \\San Raffaele}       & P-Value   \\
    \hline
     n                                                   & 110         & 46          & 64          &           \\ \hline
    \textbf{Categorical, n (\%)}                                &             &            &             &           \\ \hline
     Sex (Male)                               & 52 (47.3)   & 26 (56.5)   & 26 (40.6)   & 0.146*     \\\hline
     Bulbar Onset                      & 31 (28.2)   & 20 (43.5)   & 11 (17.2)   & \textbf{0.005}*     \\\hline
     FTD                  & 32 (29.1)   & 16 (34.8)   & 16 (25.0)   & 0.367*     \\\hline
     C9orf72                           & 7 (6.4)     & 3 (6.5)     & 4 (6.2)   & 1.000*     \\\hline
         ALS                         & 96 (87.3)   & 44 (95.7)   & 52 (81.2)   & $0.052\ddagger$     \\\hline
        \textbf{Continuous, mean (SD)}                              &             &            &             &           \\ \hline
     ALSFRS-R                             & 37.5 (7.2)  & 34.1 (8.4)  & 40.0 (5.0)  & \textbf{\ensuremath{<}0.001}\textdagger    \\\hline
     \makecell[l]{Diagnostic Delay, mo}              & 12.5 (12.0) & 11.9 (10.2) & 12.8 (13.2) & 0.682*     \\\hline
     \makecell[l]{Age at Diagnosis, yr}                 & 63.2 (11.8) & 66.2 (12.1) & 61.1 (11.1) & \textbf{0.028}$\ddagger$     \\\hline
     \makecell[l]{PRB (points/month)}   & 1.4 (1.7)   & 1.9 (2.3)   & 1.0 (1.1)   & \textbf{0.017}$\ddagger$     \\\hline
     \makecell[l]{Survival, mo}                        & 29.3 (23.2) & 24.3 (26.8) & 33.0 (19.6) & 0.066$\ddagger$     \\
    \hline
    \end{tabular}
\end{table}

Table~\ref{tab:clinical_demographics} shows the clinical features included in this analysis and statistical differences between the long and short survival groups.
The longer survival group had significantly fewer patients with FTD, a longer diagnostic delay, a younger age at diagnosis, and a slower rate of decline in ALSFRS-R .
These differences are consistent with the literature on factors associated with survival in MND~\cite{suPredictorsSurvivalPatients2021}.


Table~\ref{tab:clinical_demographics_site} shows the group differences between the two data sites.
The cohort from Ospedale San Raffaele had a significantly lower proportion of bulbar onset patients, a higher mean baseline ALSFRS-R score, a lower mean age at diagnosis, and a slower rate of decline in ALSFRS-R .
These differences suggested that the cohort from Ospedale San Raffaele had on average less aggressive disease progression at diagnosis compared to the ALS Biomarkers Study cohort.

%Harmonisation of the clinical data was not performed, as the data was collected in a similar way at both sites.
%Statistically significant differences between the sites: \textbf{List them here}
%Why didn't we do any site-specific analysis or correction?
%- Wanted to see how the model would perform in a real-world setting
%- Not enough data to do one site

\subsection{Imaging Data}
The same segmentation pipeline used in Chapter~\ref{cox_proportional_hazards_model} was used for this analysis: using SynthSeg to segment MRI conducted within 12 months before or after diagnosis.
%Regional brain volumes were extracted from the MRI using SynthSeg~\cite{billotSynthSegDomainRandomisation2021}, a modality-agnostic deep-learning segmentation tool.
%A modality agnostic tool was chosen to overcome the inconsistency in MRI protocols within the ALS Biomarkers Study and between the ALS Biomarkers Study and Ospedale San Raffaele's MND cohort.
SynthSeg returns the volumes of 33 regions of the brain, which, apart from intra-cranial volume, were used as features in this analysis.
The region volumes were z-score normalised across the entire cohort.

Two-sample t-tests were used to compare the regional brain volumes between the long and short survival groups.
The left and right volumes were summed to simplify the comparison of regional brain volumes, but were kept separate for the main machine learning analysis.
The long survival group had significantly larger volumes in the cerebellum white matter and cortex, thalamus, caudate, putamen, pallidum, brain stem, hippocampus, amygdala, accumbens area, and ventral diencephalon.
The short survival group had significantly larger volumes in the cerebrospinal fluid, 3rd ventricle, lateral ventricle, and inferior lateral ventricle.


\section{Methods}

We predicted categories of survival, short or long, by splitting the survival time at the median.
The models used in this analysis are from Fusilli, a software package developed in Chapter~\ref{fusilli_development}, to compare the performance of different multimodal data fusion methods~\cite{townendFlorencejtFusilliFusilli2024}.
We trained the models that do tabular-tabular fusion, meaning that they take two tabular datasets and combine them to predict the outcome.
There are 10 of these models in Fusilli, but we could not used the two graph-based models because the current version of Fusilli does not allow graph-based models to use held-out test sets, which was part of this analysis design.
The architecture diagrams of the models are in Appendix~\ref{appendix:fusion_model_architectures}.
Unimodal models were also trained as a benchmark for the multimodal models to investigate the added value of the multimodal data.

\begin{table}
\centering
\caption{Tabular-tabular fusion models used in this analysis, implemented using Fusilli.}
\begin{tabular}{|p{4cm}|p{7cm}|l|}
\hline
\textbf{Fusion Model} & \textbf{Description} & \textbf{Diagram} \\
\hline
Early Concatenation & The two modalities are concatenated before being input into a neural network. & Figure~\ref{fig:ConcatTabularData} \\ \hline
Intermediate Concatenation~\cite{gaoReducingUncertaintyCancer2022} & The modalities are input into separate blocks of neural network layers, then the layers are concatenated and input into a final block of layers. & Figure~\ref{fig:ConcatTabularFeatureMaps} \\ \hline
Decision Fusion & The two modalities are input into separate neural networks to obtain two prediction probabilities, and the probabilities are averaged to calculate the final output. & Figure~\ref{fig:TabularDecision} \\ \hline
Channel-wise Attention~\cite{duanmuPredictionPathologicalComplete2020} & Separate neural network layers are used for both modalities, and the intermediate layer outputs (feature maps) of one modality are multiplied with the other modality's feature maps during training.~ & Figure~\ref{fig:TabularChannelwiseAttention} \\ \hline
Crossmodal Attention~\cite{golovanevskyMultimodalAttentionbasedDeep2022} & Each modality through its own set of fully-connected layers. First, self attention is applied to each modality's intermediate feature maps, and then crossmodal attention is applied between the two modalities' feature maps. The output of the crossmodal attention is then passed through a fully-connected layer to make a prediction. & Figure~\ref{fig:TabularCrossmodalAttention} \\ \hline
Activation Fusion~\cite{chenMDFNetApplicationMultimodal2023} & Similar to intermediate concatenation, instead of the feature maps being concatenated, they are multiplied together, and passed through tanh and sigmoid activations. The first tabular modality intermediate feature map is concatenated with the activation function output and then passed through fully-connected layers to make a prediction. & Figure~\ref{fig:ActivationFusion} \\ \hline
Activation and Self-Attention~\cite{chenMDFNetApplicationMultimodal2023} & An extension to activation fusion: second tabular modality's first set of network layers include a self-attention mechanism, where the input data is multiplied with the feature maps before being fused through the activation function method. & Figure~\ref{fig:activationandselfattention} \\ \hline
Multi-channel VAE~\cite{antelmiSparseMultiChannelVariational2019} & A joint lower-dimensional subspace is created from the two modalities using a variational autoencoder set-up. The latent space is then input into a set of fully-connected layers. & Figure~\ref{fig:MCVAE} \\ \hline
\end{tabular}
\end{table}

Three experiments were conducted with different training and testing splits.
The first experiment used the data from the two studies together and extracted training and testing sets from the combined dataset.
A held-out test set is data not used for training and shows how well the model generalises to new data.
This analysis used 20\% ($n=22$) of the data as the test set.
The final experiments used a leave-site-out approach, where the models were trained on one site and tested on the other.
The second experiment was trained on the Ospeadale San Raffaele's MND cohort ($n=64$) and tested on the ALS Biomarkers Study ($n=46$), and the third experiment was trained on the ALS Biomarkers Study and tested on Ospedale San Raffaele's MND cohort.

The models were trained using 5-fold cross-validation, where the data is split into 5 sets, and the model is trained on 4 of the sets and validated on the remaining set.
A validation set is not used to train the model, but to test the performance of the model to see the training progress.
Early stopping was used to prevent overfitting on the validation set, where the model stopped training if the loss on the validation set does not improve for 5 rounds of training, or ``epochs".

% metrics
The performances metrics used to evaluate the models were balanced accuracy, area under the receiver operating characteristic curve (AUROC), precision, recall, and F1 score.
Balanced accuracy is the average of the sensitivity and specificity of the model, and is used to account for class imbalance in the data.
AUROC is the area under the receiver operating characteristic curve, which is a plot of the true positive rate against the false positive rate.
A true positive in this analysis was a correct prediction of a long survivor, and a false positive was a short survivor predicted as a long survivor.
Precision is the proportion of true positive predictions out of all positive predictions, and recall is the proportion of true positive predictions out of all actual positive samples.
The F1 score is the harmonic mean of precision and recall, and is used to balance the two metrics.

The cross-validation training resulted in 5 trained versions of each model.
The held-out test set was input into each of the these trained models to obtain the predicted survival categories.
The predicted survival categories from each fold were aggregated together to calculate the overall performance metrics for the model.


% repetitions
Due to the small sample size, initial experiments showed high variability in the model performances between reruns of the training and evaluation process.
Therefore, we employed experiment repetitions to obtain a more stable estimate of the model performance.
The training and evaluation process was repeated until the mean of the performance of the repetitions for each model converged to within 1\% of the main metric, balanced accuracy.
After the repetition performances converged, the models were compared by using the distribution of the performance metrics over the repetitions, and ranked by the mean of the balanced accuracy on the test sets.


\section{Results}
\subsection{Training together}

\begin{figure}
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.2\linewidth]{figures/both_sites_together_balanced_accuracy_violin}
    \caption{Balanced accuracies of test set and validation set when sites trained together.
    Models were ranked from best to worst by their mean balanced accuracy on the test set.}
    \label{fig:trainedtogetherviolin}
\end{figure}

\begin{table}[h]
\centering
\caption{Means and standard deviations of test metrics over repetitions of training on both sites. Bold is the best metric model.}
\label{tab:mnd_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}

 & \multicolumn{5}{c}{\textbf{Performance metric: mean (standard deviation)}} \\
\textbf{Method} & Balanced Accuracy & AUROC & Precision & Recall & F1 \\ \hline
 Early Concatenation & \textbf{0.703 (0.023)} & \textbf{0.815 (0.014)} & 0.846 (0.018) & 0.598 (0.065) & 0.698 (0.044) \\
Clinical Only & 0.680 (0.025) & 0.748 (0.052) & \textbf{0.870 (0.044)} & 0.493 (0.049) & 0.627 (0.040) \\
 Activation Function & 0.680 (0.039) & 0.744 (0.054) & 0.868 (0.048) & 0.493 (0.620) & 0.626 (0.057) \\
Activation and Attention & 0.652 (0.061) & 0.712 (0.093) & 0.772 (0.237) & 0.451 (0.158) & 0.566 (0.186) \\
Multi-channel VAE & 0.648 (0.049) & 0.727 (0.048) & 0.833 (0.075) & 0.474 (0.084) & 0.597 (0.073) \\
Intermediate Concatenation & 0.606 (0.044) & 0.663 (0.029) & 0.708 (0.030) & \textbf{0.764 (0.036)} & \textbf{0.735 (0.029)} \\
Decision Fusion & 0.589 (0.040) & 0.647 (0.042) & 0.737 (0.030) & 0.488 (0.166) & 0.567 (0.141) \\
Extracted Volumes Only & 0.583 (0.041) & 0.619 (0.038) & 0.695 (0.029) & 0.718 (0.101) & 0.703 (0.057) \\
Channelwise Attention & 0.576 (0.047) & 0.596 (0.052) & 0.730 (0.059) & 0.486 (0.193) & 0.558 (0.137) \\
Crossmodal Attention & 0.562 (0.028) & 0.610 (0.041) & 0.681 (0.025) & 0.720 (0.079) & 0.697 (0.032) \\

\end{tabular}
}
\end{table}

Table~\ref{tab:mnd_results} shows the test performances of the models as the means and standard deviations of the test metrics over the repetitions of training on both sites.
The distributions of the test and validation balanced accuracies are shown in Figure~\ref{fig:trainedtogetherviolin}.
Early concatenation performed the best with 70\% balanced accuracy and 81\% AUROC, and was the only multimodal model that performed better than the clinical unimodal model.
The clinical unimodal model came second with 68\% balanced accuracy and 75\% AUROC, and the imaging unimodal model came eighth with 58\% balanced accuracy and 60\% AUROC.

The right panel of Figure~\ref{fig:trainedtogetherviolin} shows the distributions of the validation balanced accuracies over the repetitions of training on both sites.
A trend was observed where the best performing models in the test set had less variance in the validation set performance, suggesting that the models were not overfitting to the training data.
Model overfitting could be seen with some models, such as intermediate concatenation and imaging only, which showed high validation accuracy but much lower test accuracy.

The precisions of the models were generally higher than the recalls, meaning that the models were better at accurately detecting short survivors than long survivors.

\subsection{Leave-site-out analysis}

\begin{figure}
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.2\textwidth]{figures/violinplot_site_effect}
    \caption{Effect of training on one site and testing on the other on the balanced accuracy, precision, and recall of the different models. Ordered by best to worst balanced accuracy of trained together.}
    \label{fig:site_effect}
\end{figure}

Figure~\ref{fig:site_effect} shows the effect of training on one site and testing on the other on the balanced accuracy, precision, and recall of the different models.
The models were ordered by their balanced accuracy when trained on both sites together, for comparison with the first experiment.
The model performance ordering was similar to the first experiment, but early concatenation did not perform as well as when trained on both sites together.
Training on the ALS Biomarkers Study generally resulted in worse performance than training on Ospedale San Raffaele, when looking at overall balanced accuracy.
%When considering precision only, training on the ALS Biomarkers Study resulted in better performance, but when considering recall only, training on Ospedale San Raffaele resulted in better performance.
The ALS Biomarkers Study-trained models were better at detecting long survivors than the Ospedale San Raffaele-trained models (higher precision), but the Ospedale San Raffaele-trained models were better at detecting short survivors (higher recall).

\section{Discussion}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/tsne_plot}
    \caption{TSNE with two components of the clinical, imaging, and multimodal data, split by survival category to illustrate the separability of the data.}
    \label{fig:tsne}
\end{figure}

K-fold cross validation and stability repetitions aimed to improve model performance and clarify the relationsip between model type and performance.
However, the highest model accuracy reached only 70\%, with the highest AUROC at 81\%.
In contrast, a similar study achieved an 84\% accuracy using a multimodal deep learning model trained on a larger sample of 83 patients, compared to our 70 patients~\cite{vanderburghDeepLearningPredictions2017}.
Differences in outcome categories and imaging features between studies may explain the performance gap.
For instance, van der Burgh and colleagues used structural connectivity matrices and cortical thicknesses, potentially more effective for prognosis prediction.
Moreover, their data focused solely on ALS patients, possibly resulting in a more homogeneous dataset conducive to prediction.
Future investigations may benefit from excluding MND subtypes other than ALS, a common practice in prior machine learning studies, to further explore prognosis prediction with increased sample size.


%The most similar study to this analysis used a multimodal deep learning model, most similar to the Fusilli intermediate concatenation model, to predict short, medium, and long survival categories in ALS with an accuracy of 84\%, trained on $n=83$ patients~\cite{vanderburghDeepLearningPredictions2017}.
%The models in our analysis were trained on $n=70$ patients.
%The lower performance of the models in this analysis could be due to the smaller sample size, the different outcome categories, or the different data used in the analysis.
%Moreover, van der Burgh and colleagues used different imaging features: structural connectivity matrices, and in addition to subcortical volumes, they used cortical thicknesses.
%This difference in performance suggests that the imaging features used in this analysis may not be as useful for predicting prognosis as the imaging features used by van der Burgh and colleagues, or perhaps that survival in MND is easier to predict when split into three categories rather than two.
%However, their data included only ALS patients, rather than MND patients, which may have made the data more homogenous and easier to predict.
%With increased sample size, future work will be to look at the effect of excluding patients who have an MND-subtype other than ALS, which is an common exclusion criteria in previous ML studies.

Figure~\ref{fig:tsne} illustrates the data separability in two dimensions using t-distributed stochastic neighbour embedding (TSNE) for clinical, imaging, and multimodal features.
Fuzzy borders between long and short survivors indicate limited separability, potentially contributing to the lower model performance.

In the both-site experiment, only the early concatenation multimodal model surpassed the clinical unimodal model, albeit marginally, with a 2\% balanced accuracy and 6\% AUROC improvement.
This suggests clinical data's primary importance in predicting MND survival, with imaging data offering minimal value.
The poor performance of most multimodal models and the imaging unimodal model further corroborated this.
Moreover, evidence of overfitting in the validation set of the both-site experiment suggests that models struggled to discern meaningful relationships amidst data noise.
With 32 features in imaging data compared to 9 in clinical data, the models overfitting to imaging data likely contributed to poor multimodal model performance.
%This implies that clinical data was the most important for predicting survival in MND, and that the imaging data did not add much value to the prediction.
%Further corroborating this, the low performance of most multimodal models' performances might suggest that features extracted from the imaging data were not useful for predicting prognosis.
%This was further corroborated by the low performance of the imaging unimodal model.
%However, the evidence of overfitting in the validation set of the both-site experiment suggests that the models were not able to learn the relationship between the features and the outcome, and were learning the noise in the data.
%Since the imaging data had a larger number of features (32) compared to the clinical data (9), the models may have been overfitting to the imaging data, which may have led to the poor performance of the multimodal models.
Therefore, it is possible with a higher sample size, that the value of the imaging data in predicting prognosis could be uncovered more clearly.


Deep learning models vary in the number of learnable parameters, indicating their complexity.
More complex models typically demand more data to train effectively and prevent overfitting.
The intermediate concatenation model was the best performing model when measured by recall and F1 score, and has more trainable parameters than the overall best model, early concatenation, with 34,200 learnable parameters compared to 27,500 in early concatenation.
%Figure~\ref{fig:trainedtogetherviolin} shows that the intermediate concatenation model had the highest validation balanced accuracy of approximately 95\% in the both-site experiment, but the test balanced accuracy was only 60\%, suggesting overfitting to the training data.
In the both-site experiment, the intermediate concatenation model achieved the highest validation balanced accuracy at around 95\%, yet its test balanced accuracy was only 60\%, indicating overfitting to the training data.
Although no statistically significant relationship was found between the number of learnable parameters and test performance, model complexity can also be inferred from architecture.
%There was no clear relationship between model architecture and performance, but t
The two attention-based models performed the worst, which may suggest that attention mechanisms are not suited to the data in its current state.

Training on one dataset and testing on the other assessed model generalisability and the impact of reduced training sample size on performance.
Unsurprisingly, the dataset with the higher sample size, Ospedale San Raffaele, performed better than the dataset with the lower sample size, the ALS Biomarkers Study.
Moreover, the ALS Biomarkers Study data came from three different sites, which may have introduced noise into the data.
Figure~\ref{fig:site_effect} shows that the models trained on the ALS Biomarkers Study were better at detecting long survivors than the models trained on Ospedale San Raffaele.
Although the two datasets did not have statistically significant differences in patient survival times, the ALS Biomarkers Study had a higher proportion of long survivors than Ospedale San Raffaele, which may have led to the models being better at detecting long survivors when trained on the ALS Biomarkers Study data.
However, drawing definitive conclusions is challenging due to limited sample sizes of $n=64$ and $n=46$, especially when dealing with high-dimensional multimodal data, which may affect deep learning model training.

Most models exhibited higher precision than recall for short survivors compared to long survivors across both datasets, as indicated by Table~\ref{tab:mnd_results}.
This discrepancy was unexpected, given the balanced labels achieved by splitting on the median survival time.
Further investigation is warranted to determine whether the models are failing to predict specific subjects or if there are other factors contributing to their poor performance in detecting long survivors.

\subsection{Limitations}
The largest limitation of this analysis was the small sample size, which may have led to the models overfitting to the training data and not generalising well to the test data.
Multimodal models that are not based on deep learning were not included in this analysis, which may have performed better with the small sample size.
There have been conflicting results on whether deep learning is the best technique for MND prognosis prediction with clinical data~\cite{pancottiDeepLearningMethods2022, papaizEnsembleimbalancebasedClassificationAmyotrophic2024}, although it has shown promise in multimodal settings~\cite{vanderburghDeepLearningPredictions2017, meierConnectomeBasedPropagation2020}
Future work will compare the performances of deep learning models to non-deep learning models.

Another aspect to the small sample size was the lack of a completely external test set.
Although the leave-site-out analysis was used to investigate the generalisability of the models to new data, the small sample sizes led to difficulty in concluding what was causing the differences in model performance.
With the addition of more sites and more patients in the future, the generalisability of the models to new data can be more clearly understood.

As the data in this chapter was also used in Chapter~\ref{cox_proportional_hazards_model}, many of the limitations in relation to the data were the same.
The extracted brain volumes used in this analysis may not have been the most relevant features for predicting prognosis in MND.
Although the volumes showed significant associations with survival in Chapter~\ref{cox_proportional_hazards_model}, performance may be improved with more curated features that have shown prognostic value, such as more detailed brain-stem volumes~\cite{milellaMedullaOblongataVolume2022} or cortical thicknesses~\cite{burghMultimodalLongitudinalStudy2020,dieckmannCorticalSubcorticalGrey2022}.
The sample size in this analysis was too small to explore using the whole MRI image as input to the models, but this may be a useful next step to see if the models can learn more from the raw MRI data than the extracted volumes.

Moreover, the clinical features could have been more granular, such as including the individual components of the ALSFRS-R, rather than the total score, which has shown to be useful in prognosis prediction~\cite{hothornRandomForest4LifeRandomForest2014}.
Unfortunately, the Ospedale San Raffaele cohort did not have the individual components of the ALSFRS-R, so this was not possible in this analysis.

Finally, no hyperparameter tuning was done in this analysis, which may have led to suboptimal model performance.
Future work will look at improving model performance through optimising architecture features such as layer sizes and learning rates.

\section{Conclusion}

This chapter detailed a multimodal data fusion analysis of clinical and imaging data in MND prognosis prediction.
Eight multimodal models were compared to each other and to unimodal models.
The best performing model was early concatenation, which combined the clinical and imaging data through simple concatenation before being input into a neural network.
However, early concatenation was the only multimodal model that outperformed the clinical unimodal model, suggesting that the imaging data did not add much value to the prediction of prognosis in MND.
Further work will look at the effect of including more sites and more patients in the analysis, as well as the effect of different imaging features on model performance.
