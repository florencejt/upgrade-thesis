\chapter{Comparison of Multimodal Data Fusion Models for Predicting Survival Classification in Motor Neuron Disease}
\label{fusilli_on_mnd}

\section{Introduction}
Only one deep learning model has been used to predict survival in MND through clinical data and neuroimaging data~\cite{vanderburghDeepLearningPredictions2017}.
This lack of research in multimodal data fusion in MND makes it difficult to conclude whether multimodal data is useful in predicting survival in MND .
Additionally, we have shown that many different multimodal data fusion methods have been developed for other research applications.
In Chapter~\ref{fusilli_development}, we developed a package called Fusilli to compare the performance of multimodal data fusion methods, approximately half of which are designed to combine two tabular modalities.

In this chapter, we will use Fusilli to compare the performance of different tabular-tabular multimodal data fusion methods on predicting survival in MND patients using clinical and imaging extracted features data.
The aims of this work are to, firstly, assess the effect of different data fusion model architectures on prognostic performance, and secondly, to assess the value of baseline clinical and neuroimaging data in MND prognosis prediction.

\section{Data}

The data used in this analysis is from two studies: University College London Queen's Square Institute of Neurology's ALS Biomarkers Study~\cite{UKMNDCSG} and Ospedale San Raffaele's MND cohort.
Both of these datasets contain clinical data from the diagnostic visit and brain MRI data.

For patients to be included in the analysis, they must have a diagnosis of MND and have an outcome of interest (death or tracheostomy).
In the ALS Biomarkers Study, the outcome of interest is death, whereas in the Ospedale San Raffaele's MND cohort, the outcome of interest is death or tracheostomy.
Unfortunately, the Ospedale San Raffaele's MND cohort does not specify the date of death, so we have assumed that the date of death is the date of tracheostomy.

Patients in this analysis must have non-missing data for age at diagnosis, sex, date of diagnosis, date of death, date of symptom onset, site of onset, and baseline ALSFRS-R .
Additionally, patients must have a T1-weighted or T2-weighted MRI within 12 months before or after their date of diagnosis.

The final dataset contains 110 MND patients.
The patients in the cohort were split into two groups based on the median survival time: short survival group (less than 24 months) and long survival group (more than 24 months).

\subsection{Clinical Data}

The clinical variables chosen to be included in the analysis are based on the variables used in the ENCALS model~\cite{westenengPrognosisPatientsAmyotrophic2018} described in Chapter~\ref{literature_review}.
El Escorial criteria and FVC were not included in the analysis as they were not available in the Ospedale San Raffaele's MND cohort.
Features with missing data after the inclusion criteria were applied were features of FTD and presence of C9orf72 mutation.
Where these features were missing, they were assumed to be negative.
Moreover, where the MND type was missing, it was assumed to be ALS, as it is the most common.


\begin{table}
    \centering
    \caption{Differences in clinical demographics between the long and short survival groups. PRB is progression rate to baseline, calculated as the rate of decline of ALSFRS-R between symptom onset and diagnosis.\\
    *Chi-square test, \textdagger Fisher's exact test, $\ddagger$ Two-sample t-test.}
    \label{tab:clinical_demographics}
    \begin{tabular}{|p{4.3cm}|llll|}
    \hline
                                                        & \textbf{Overall}     & \textbf{Short}        & \textbf{Long}         & \textbf{P-Value}   \\
    \hline
     n                                                  & 110         & 55         & 55          &           \\ \hline
    \textbf{Categorical, n (\%)}                                &             &            &             &           \\ \hline
     Sex (Male)                                     & 52 (47.3)   & 27 (49.1)  & 25 (45.5)   & 0.849*     \\ \hline
     Bulbar Onset                          & 31 (28.2)   & 20 (36.4)  & 11 (20.0)  & 0.090*     \\\hline
     FTD                      & 32 (29.1)   & 24 (43.6)  & 8 (14.5)   & \textbf{0.002}*     \\\hline
     C9orf72                             & 7 (6.4)     & 2 (3.6)    & 5 (9.1)   & 0.438\textdagger     \\\hline
    ALS                               & 96 (87.3)   & 51 (92.7)  & 45 (81.8)   & 0.153*     \\\hline
    \textbf{Continuous, mean (SD)}                              &             &            &             &           \\ \hline
     ALSFRS-R                                & 37.5 (7.2)  & 36.3 (7.2) & 38.7 (7.0)  & $0.081\ddagger$    \\\hline
     \makecell[l]{Diagnostic Delay, mo}                 & 12.5 (12.0) & 10.0 (9.8) & 14.9 (13.4) & \textbf{0.031}$\ddagger$     \\\hline
     \makecell[l]{Age at Diagnosis, yr}                   & 63.2 (11.8) & 69.1 (9.1) & 57.3 (11.3) & \textbf{\ensuremath{<}0.001}$\ddagger$    \\\hline
     \makecell[l]{PRB (points/month)}       & 1.4 (1.7)   & 2.0 (2.2)  & 0.9 (0.7)   & \textbf{0.001}$\ddagger$      \\\hline
     \makecell[l]{Survival, mo}                          & 29.3 (23.2) & 12.3 (6.4) & 46.4 (21.3) & \textbf{\ensuremath{<}0.001}$\ddagger$     \\\hline
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Differences in clinical demographics between the two data sites: the ALS Biomarkers Study from University College London and Ospedale San Raffaele.
    PRB is progression rate to baseline, calculated as the rate of decline of ALSFRS-R between symptom onset and diagnosis.\\
    *Chi-square test, \textdagger Fisher's exact test, $\ddagger$ Two-sample t-test.}
    \label{tab:clinical_demographics_site}
    \begin{tabular}{|p{4.3cm}|llll|}
    \hline
                                                       & Overall     & \makecell[l]{ALS Biomarkers \\ Study}       & \makecell[l]{Ospedale \\San Raffaele}       & P-Value   \\
    \hline
     n                                                   & 110         & 46          & 64          &           \\ \hline
    \textbf{Categorical, n (\%)}                                &             &            &             &           \\ \hline
     Sex (Male)                               & 52 (47.3)   & 26 (56.5)   & 26 (40.6)   & 0.146*     \\\hline
     Bulbar Onset                      & 31 (28.2)   & 20 (43.5)   & 11 (17.2)   & \textbf{0.005}*     \\\hline
     FTD                  & 32 (29.1)   & 16 (34.8)   & 16 (25.0)   & 0.367*     \\\hline
     C9orf72                           & 7 (6.4)     & 3 (6.5)     & 4 (6.2)   & 1.000*     \\\hline
         ALS                         & 96 (87.3)   & 44 (95.7)   & 52 (81.2)   & $0.052\ddagger$     \\\hline
        \textbf{Continuous, mean (SD)}                              &             &            &             &           \\ \hline
     ALSFRSr                             & 37.5 (7.2)  & 34.1 (8.4)  & 40.0 (5.0)  & \textbf{\ensuremath{<}0.001}\textdagger    \\\hline
     \makecell[l]{Diagnostic Delay, mo}              & 12.5 (12.0) & 11.9 (10.2) & 12.8 (13.2) & 0.682*     \\\hline
     \makecell[l]{Age at Diagnosis, yr}                 & 63.2 (11.8) & 66.2 (12.1) & 61.1 (11.1) & \textbf{0.028}$\ddagger$     \\\hline
     \makecell[l]{PRB (points/month)}   & 1.4 (1.7)   & 1.9 (2.3)   & 1.0 (1.1)   & \textbf{0.017}$\ddagger$     \\\hline
     \makecell[l]{Survival, mo}                        & 29.3 (23.2) & 24.3 (26.8) & 33.0 (19.6) & 0.066$\ddagger$     \\
    \hline
    \end{tabular}
\end{table}

Table~\ref{tab:clinical_demographics} shows the clinical features included in this analysis and statistical differences between the long and short survival groups.
The longer survival group had significantly fewer patients with FTD, a longer diagnostic delay, a younger age at diagnosis, and a slower rate of decline in ALSFRS-R .
These differences are consistent with the literature on factors associated with survival in MND~\cite{suPredictorsSurvivalPatients2021}.


Table~\ref{tab:clinical_demographics_site} shows the group differences between the two data sites.
The cohort from Ospedale San Raffaele had a significantly lower proportion of bulbar onset patients, a higher mean baseline ALSFRS-R score, a lower mean age at diagnosis, and a slower rate of decline in ALSFRS-R .
These differences, in combination with survival factors suggested in the literature, suggest that the cohort from Ospedale San Raffaele have had less aggressive disease progression at diagnosis compared to the ALS Biomarkers Study cohort.

Statistically significant differences between the sites: \textbf{List them here}
Why didn't we do any site-specific analysis or correction?
- Wanted to see how the model would perform in a real-world setting
- Not enough data to do one site

\subsection{Imaging Data}
The same segmentation pipeline used in Chapter~\ref{cox_proportional_hazards_model} was used for this analysis also: using SynthSeg to segment MRI conducted within 12 months before or after diagnosis.
%Regional brain volumes were extracted from the MRI using SynthSeg~\cite{billotSynthSegDomainRandomisation2021}, a modality-agnostic deep-learning segmentation tool.
%A modality agnostic tool was chosen to overcome the inconsistency in MRI protocols within the ALS Biomarkers Study and between the ALS Biomarkers Study and Ospedale San Raffaele's MND cohort.
SynthSeg returns the volumes of 33 regions of the brain, which, apart from intra-cranial volume, were used as features in this analysis.
The region volumes were z-score normalised across the entire cohort.

The left and right volumes were summed to simplify the comparison of regional brain volumes between the long and short survival groups, but the left and right volumes were kept separate for the main analysis.
Two-sample t-tests were used to compare the regional brain volumes between the long and short survival groups.
The long survival group had significantly larger volumes in the cerebellum white matter and cortex, thalamus, caudate, putamen, pallidum, brain stem, hippocampus, amygdala, accumbens area, and ventral diencephalon.
The short survival group had significantly larger volumes in the cerebrospinal fluid, 3rd ventricle, lateral ventricle, and inferior lateral ventricle.


\section{Methods}

We predicted categories of survival, short or long, by splitting the survival time at the median.
The median was used, rather than a clinically relevant cut-off, as the sample size was small and having balanced classes reduces the risk of overfitting.
Moreover, the median survival time in our data of 24 months is a reasonable cut-off for short and long survival in MND~\cite{feldmanAmyotrophicLateralSclerosis2022}.


The models used in this analysis are from Fusilli, a package developed in Chapter~\ref{fusilli_development} to compare the performance of different multimodal data fusion methods.
We trained the models that do tabular-tabular fusion, meaning that they take two tabular datasets and combine them to predict the outcome.
There are 10 tabular-tabular models in Fusilli, but we could not used the two graph-based models because the current version of Fusilli does not allow graph-based models to use held-out test sets.
The architecture diagrams of the models are in Appendix~\ref{appendix:fusion_model_architectures}.
Unimodal models were also trained as a benchmark for the multimodal models to investigate the added value of the multimodal data.

\begin{table}
\centering
\caption{Tabular-tabular fusion models used in this analysis, implemented using Fusilli.}
\begin{tabular}{|p{4cm}|p{7cm}|l|}
\hline
\textbf{Fusion Model} & \textbf{Description} & \textbf{Diagram} \\
\hline
Early Concatenation & The two modalities are concatenated before being input into a neural network. & Figure~\ref{fig:ConcatTabularData} \\ \hline
Intermediate Concatenation~\cite{gaoReducingUncertaintyCancer2022} & The modalities are input into separate blocks of neural network layers, then the layers are concatenated and input into a final block of layers. & Figure~\ref{fig:ConcatTabularFeatureMaps} \\ \hline
Decision Fusion & The two modalities are input into separate neural networks to obtain two prediction probabilities, and the probabilities are averaged to calculate the final output. & Figure~\ref{fig:TabularDecision} \\ \hline
Channel-wise Attention~\cite{duanmuPredictionPathologicalComplete2020} & Separate neural network layers are used for both modalities, and the intermediate layer outputs (feature maps) of one modality are multiplied with the other modality's feature maps during training.~ & Figure~\ref{fig:TabularChannelwiseAttention} \\ \hline
Crossmodal Attention~\cite{golovanevskyMultimodalAttentionbasedDeep2022} & Each modality through its own set of fully-connected layers. First, self attention is applied to each modality's intermediate feature maps, and then crossmodal attention is applied between the two modalities' feature maps. The output of the crossmodal attention is then passed through a fully-connected layer to make a prediction. & Figure~\ref{fig:TabularCrossmodalAttention} \\ \hline
Activation Fusion~\cite{chenMDFNetApplicationMultimodal2023} & Similar to intermediate concatenation, instead of the feature maps being concatenated, they are multiplied together, and passed through tanh and sigmoid activations. The first tabular modality intermediate feature map is concatenated with the activation function output and then passed through fully-connected layers to make a prediction. & Figure~\ref{fig:ActivationFusion} \\ \hline
Activation and Self-Attention~\cite{chenMDFNetApplicationMultimodal2023} & An extension to activation fusion: second tabular modality's first set of network layers include a self-attention mechanism, where the input data is multiplied with the feature maps before being fused through the activation function method. & Figure~\ref{fig:activationandselfattention} \\ \hline
Multi-channel VAE~\cite{antelmiSparseMultiChannelVariational2019} & A joint lower-dimensional subspace is created from the two modalities using a variational autoencoder set-up. The latent space is then input into a set of fully-connected layers. & Figure~\ref{fig:MCVAE} \\ \hline
\end{tabular}
\end{table}

Three experiments were conducted with different training and testing splits.
The first experiment mixed the two datasets together and extracted training and testing sets from the combined dataset.
A held-out test set is data not used for training and shows how well the model generalises to new data.
This analysis used 20\% ($n=22$) of the data as the test set.
The final experiments used a leave-site-out approach, where the models were trained on one site and tested on the other.
The second experiment was trained on the Ospeadale San Raffaele's MND cohort ($n=64$) and tested on the ALS Biomarkers Study ($n=46$), and the third experiment was trained on the ALS Biomarkers Study and tested on Ospedale San Raffaele's MND cohort.

The models were trained using 5-fold cross-validation, where the data is split into 5 sets, and the model is trained on 4 of the sets and validated on the remaining set.
The validation set is not used to train the model, but to test the performance of the model to see the training progress.
Early stopping was used to prevent overfitting on the validation set, where the model stops training if the loss on the validation set does not improve for 5 rounds of training, or ``epochs".

% metrics
The performances metrics used to evaluate the models were balanced accuracy, area under the receiver operating characteristic curve (AUROC), precision, recall, and F1 score.
Balanced accuracy is the average of the sensitivity and specificity of the model, and is used to account for class imbalance in the data.
AUROC is the area under the receiver operating characteristic curve, which is a plot of the true positive rate against the false positive rate.
A true positive in this analysis was a correct prediction of a long survivor, and a false positive was a short survivor predicted as a long survivor.
Precision is the proportion of true positive predictions out of all positive predictions, and recall is the proportion of true positive predictions out of all actual positive samples.
The F1 score is the harmonic mean of precision and recall, and is used to balance the two metrics.

The cross-validation training resulted in 5 trained versions of each model.
The held-out test set was input into each of the these trained models to obtain the predicted survival categories.
The predicted survival categories from each fold were aggregated together to calculate the overall performance metrics for the model.


% repetitions
Due to the small sample size, initial experiments showed high variability in the model performances between reruns of the training and evaluation process.
Therefore, we employed experiment repetitions to stabilise the performance metrics.
The training and evaluation process was repeated until the mean of the performance of the repetitions for each model converged to a stable value (within 1\% of the main metric: balanced accuracy).
After the repetition performances converged, the models were compared by using the distribution of the performance metrics over the repetitions, and ranked by the mean of the balanced accuracy on the test sets.


\section{Results}
\subsection{Training together}

\begin{figure}
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.2\linewidth]{figures/both_sites_together_balanced_accuracy_violin}
    \caption{Balanced accuracies of test set and validation set when sites trained together.
    Models were ranked from best to worst by their mean balanced accuracy on the test set.}
    \label{fig:trainedtogetherviolin}
\end{figure}

\begin{table}[h]
\centering
\caption{Means and standard deviations of test metrics over repetitions of training on both sites. Bold is the best metric model.}
\label{tab:mnd_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}

 & \multicolumn{5}{c}{\textbf{Performance metric: mean (standard deviation)}} \\
\textbf{Method} & Balanced Accuracy & AUROC & Precision & Recall & F1 \\ \hline
 Early Concatenation & \textbf{0.703 (0.023)} & \textbf{0.815 (0.014)} & 0.846 (0.018) & 0.598 (0.065) & 0.698 (0.044) \\
Clinical Only & 0.680 (0.025) & 0.748 (0.052) & \textbf{0.870 (0.044)} & 0.493 (0.049) & 0.627 (0.040) \\
 Activation Function & 0.680 (0.039) & 0.744 (0.054) & 0.868 (0.048) & 0.493 (0.620) & 0.626 (0.057) \\
Activation and Attention & 0.652 (0.061) & 0.712 (0.093) & 0.772 (0.237) & 0.451 (0.158) & 0.566 (0.186) \\
Multi-channel VAE & 0.648 (0.049) & 0.727 (0.048) & 0.833 (0.075) & 0.474 (0.084) & 0.597 (0.073) \\
Intermediate Concatenation & 0.606 (0.044) & 0.663 (0.029) & 0.708 (0.030) & \textbf{0.764 (0.036)} & \textbf{0.735 (0.029)} \\
Decision Fusion & 0.589 (0.040) & 0.647 (0.042) & 0.737 (0.030) & 0.488 (0.166) & 0.567 (0.141) \\
Extracted Volumes Only & 0.583 (0.041) & 0.619 (0.038) & 0.695 (0.029) & 0.718 (0.101) & 0.703 (0.057) \\
Channelwise Attention & 0.576 (0.047) & 0.596 (0.052) & 0.730 (0.059) & 0.486 (0.193) & 0.558 (0.137) \\
Crossmodal Attention & 0.562 (0.028) & 0.610 (0.041) & 0.681 (0.025) & 0.720 (0.079) & 0.697 (0.032) \\

\end{tabular}
}
\end{table}

Table~\ref{tab:mnd_results} shows the test performances of the models as the means and standard deviations of the test metrics over the repetitions of training on both sites.
The distributions of the test and validation balanced accuracies are shown in Figure~\ref{fig:trainedtogetherviolin}.
Early concatenation performed the best with 70\% balanced accuracy and 81\% AUROC, and was the only multimodal model that performed better than the clinical unimodal model.
The clinical unimodal model came second with 68\% balanced accuracy and 75\% AUROC, and the imaging unimodal model came eighth with 58\% balanced accuracy and 60\% AUROC.

The right panel of Figure~\ref{fig:trainedtogetherviolin} shows the distributions of the validation balanced accuracies over the repetitions of training on both sites.
A trend of the better test-performing models having validation accuracies with less variance was observed.
Model overfitting could be seen with some models, such as intermediate concatenation and imaging only, which showed high validation accuracy but much lower test accuracy.

The metrics other than balanced accuracy showed that the precisions of the models were generally higher than the recalls, meaning that the models were better at accurately detecting short survivors than long survivors.

\subsection{Leave-site-out analysis}

\begin{figure}
    \centering
    \hspace*{-2cm}
    \includegraphics[width=1.2\textwidth]{figures/violinplot_site_effect}
    \caption{Effect of training on one site and testing on the other on the balanced accuracy, precision, and recall of the different models. Ordered by best to worst balanced accuracy of trained together.}
    \label{fig:site_effect}
\end{figure}

Figure~\ref{fig:site_effect} shows the effect of training on one site and testing on the other on the balanced accuracy, precision, and recall of the different models.
The models were ordered by their balanced accuracy when trained on both sites together, for comparison with the first experiment.
The model performance ordering was similar to the first experiment, but early concatenation did not perform as well as it did when trained on both sites together.
Training on the ALS Biomarkers Study generally resulted in better performance than training on Ospedale San Raffaele, when looking at overall balanced accuracy.
When considering precision only, training on the ALS Biomarkers Study resulted in better performance, but when considering recall only, training on Ospedale San Raffaele resulted in better performance.


\section{Discussion}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/tsne_plot}
    \caption{TSNE with two components of the clinical, imaging, and multimodal data, split by survival category to illustrate the separability of the data.}
    \label{fig:tsne}
\end{figure}

K-fold cross validation and stability repetitions were used to try to improve the model performance and obtain a clearer picture of the relationsip between model type and performance.
However, the highest model accuracy was 70\% and the highest AUROC was 81\%.
Comparing these results to the most similar study,
The most similar study to this analysis used a multimodal deep learning model, most similar to the Fusilli intermediate concatenation model, to predict short, medium, and long survival categories in ALS with an accuracy of 84\%, trained on $n=83$ patients~\cite{vanderburghDeepLearningPredictions2017}.
The models in our analysis were trained on $n=70$ patients.
The lower performance of the models in this analysis could be due to the smaller sample size, the different outcome categories, or the different data used in the analysis.
Moreover, van der Burgh and colleagues used different imaging features: structural connectivity matrices, and in addition to subcortical volumes, they used cortical thicknesses.
This difference in performance suggests that the imaging features used in this analysis may not be as useful for predicting prognosis as the imaging features used by van der Burgh and colleagues, or perhaps that survival in MND is easier to predict when split into three categories rather than two.
However, their data included only ALS patients, rather than MND patients, which may have made the data more homogenous and easier to predict.
With increased sample size in the future, future work will be to look at the effect of including non-ALS MND patients in the analysis, as opposed to only ALS patients, which is more common in previous studies.
Furthermore, Figure~\ref{fig:tsne} showed the separability of the data in two dimensions using t-distributed stochastic neighbour embedding (TSNE) for clinical, imaging, and multimodal features.
Fuzzy borders between long and short survivors were observed, suggesting that the data is not very separable in two dimensions, which may explain the lower performance of the models.

% start with the biggest result:
The only multimodal model that outperformed the clinical unimodal model in the both-site experiment was early concatenation, and it outperformed by a small margin of 2\% balanced accuracy and 6\% AUROC.
This implied that clinical data was the most important for predicting survival in MND, and that the imaging data did not add much value to the prediction.
%Early concatenation, also called ``early fusion", is one of the simplest ways to combine two modalities, where both types of data are concatenated before being input into a neural network.
The low performance of most multimodal performance might suggest that features extracted from the imaging data were not useful for predicting prognosis.
This was further corroborated by the low performance of the imaging unimodal model.
However, the evidence of overfitting in the validation set of the both-site experiment suggests that the models were not able to learn the relationship between the features and the outcome, and were learning the noise in the data.
Since the imagign data had a larger number of features (32) compared to the clinical data (9), the models may have been overfitting to the imaging data, which may have led to the poor performance of the multimodal models.
Therefore, it is possible with a higher sample size, that the value of the imaging data in predicting prognosis could be uncovered more clearly.

Although each of the multimodal models had an 41 input features (the clinical and imaging features), the models had different numbers of learnable parameters, which define model complexity.
Complex models typically require more data to train to a high generalisable performance and avoid overfitting.


% a high ratio of features to samples is not good for training models, as the models can overfit to the training data
% what is surprising, is that the early concatenation model has an feature dimension of the imaging and the clinical data, which is 41 features, which is a high ratio of features to samples, so why did it perform the best
% potentially there is a confounding effect of small sample size to feature ratio, where the model is not able to learn the relationship between the features and the outcome, and the model is learning the noise in the data, which makes the complex models perform worse than the simple ones which aren't as able to learn the noise
% higher sample size is needed to reduce the effect of overfitting on small samples and uncover the real relationship between complexity and performance, and the value of multimodal data in predicting prognosis

% intermediate concatenation also performed well with the highest recall and F1 score.
% Slightly more complex than early concatenation, with 34.2k learnable parameters compared to 27.5k in early concatenation.
% This shows increased model complexity might explain t the validation balanced accuracy, where intermediate concatenation was one of the most overfit models with all validation accuracies between 90-100\% but the test balanced accuracy was 60\%.
% Looking at the different metrics is important to see the overall performance of the models

% improved precision over recall for 7 out of 10 of the models suggests that the models are better at detecting short survivors than long survivors.
% why could this be? since the labels are balanced by splitting on the mean, the models should be equally good at detecting short and long survivors
% further work should be done to see if the models are having a hard time to predict specific subjects that maybe have a specific feature that is hard to predict

% generally, there was no clear relationship between architecture type and performance, but the two attention-based models performed the worst, which may suggest that attention mechanisms are not suited to the data in its current state


Leave-site-out:
- Training on the higher sample size site (Milan) performs better on balanced accuracy than training on the lower sample size site (ALS Biomarkers Study), which is expected.
- Early concatenation doesn't perform as well as it does when trained on both sites together, suggesting that it may be overfitting on the Milan data
- Statistically significant differences between the sites in Table~\ref{tab:clinical_demographics_site} showed that the Milan patients had different clinical features to the Essex patients, which may have meant the results were ungeneralisable to ALS Biomarkers Study.
- However, the overall results are similar to the trained together results, either suggesting that the models are generalisable to the ALS Biomarkers Study data or that model architecture is more important than the data used to train it.
- moreover, the als biomarkers study data came from 3 different sites, so the data is more heterogeneous than the Milan data, which came from one site
- this may have led to noisy data in the als biomarkers study data, which may have affected the model performance when tested on milan
- generally, sample sizes of 64 and 46 are not sufficient for training deep learning models, especially when the number of features is high, as in the imaging data
- with more sites and higher sample sizes, doing leave-site-out analysis will be good to test the generalisability of models, and this is a proof of concept that differetn data leads to different outcomes, so it is important to test the models on different data

\subsection{Limitations}
\begin{itemize}
    \item Limitations on sample size
    \item Evaluating on validation set rather than a completely external test set
    \item Predictive task of classification rather than regression: what if we used a regression task instead? Would that be more useful? It's a harder task so may require more data
    \item Limitation on using extracted brain volumes rather than raw MRI: what if the regions we've chosen aren't the most important ones? Subcortical regions have shown to have a role in MND, but we haven't included them here. *Look this up - the thalamus stuff*. However, whole image may introduce bias because further progressed patients may have worse quality scans.
    \item Two sites put together without harmonisation
    \item Using whole ALSFRS-R rather than individual components - not possible to get with Milan data
    \item Needs more hyperparameter tuning of the different models to see if they can be improved. Next steps would be to test different network architectures and hyperparameters to see if the results can be improved.
\end{itemize}

\section{Conclusion}
First look at multimodal data fusion in MND. What does it mean? What are the implications? What are the next steps?
\begin{itemize}
    \item If imaging + clinical is useful
    \begin{itemize}
        \item Let's add modalities
        \item Let's mix up the imaging preprocessing: DTI? Sub-cortical segmentation?
    \end{itemize}
    \item If imaging + clinical isn't useful
    \begin{itemize}
        \item Let's swap out the imaging for other modalities
        \item Let's try different machine learning models
        \item Let's mix up the imaging preprocessing: DTI? Sub-cortical segmentation?
    \end{itemize}
\end{itemize}
